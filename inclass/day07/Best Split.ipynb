{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learning Choosing the Best Split\n",
    "\n",
    "In this notebook you will be writing a Python function to compute the best possible split for a decision tree given some points in the x, y plane and some binary targets.\n",
    "\n",
    "For starters, we will generate a random decision tree and then plot the points.  The binary targets are noisy realizations of the true underlying decision function (with noise specified by the variable `noise`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if y > 227.000000:\n",
      "    if y > 447.000000:\n",
      "        if x > 94.000000:\n",
      "            if x > 291.000000:\n",
      "                return True\n",
      "            else:\n",
      "                return False\n",
      "        else:\n",
      "            return False\n",
      "    else:\n",
      "        if x > 432.000000:\n",
      "            if y > 301.000000:\n",
      "                if x > 479.000000:\n",
      "                    if x > 499.000000:\n",
      "                        if y > 327.000000:\n",
      "                            return True\n",
      "                        else:\n",
      "                            return False\n",
      "                    else:\n",
      "                        return False\n",
      "                else:\n",
      "                    return False\n",
      "            else:\n",
      "                return True\n",
      "        else:\n",
      "            if x > 111.000000:\n",
      "                if x > 208.000000:\n",
      "                    if x > 415.000000:\n",
      "                        return True\n",
      "                    else:\n",
      "                        return False\n",
      "                else:\n",
      "                    if y > 419.000000:\n",
      "                        if y > 427.000000:\n",
      "                            return True\n",
      "                        else:\n",
      "                            if x > 147.000000:\n",
      "                                return False\n",
      "                            else:\n",
      "                                if y > 424.000000:\n",
      "                                    return True\n",
      "                                else:\n",
      "                                    if x > 141.000000:\n",
      "                                        return False\n",
      "                                    else:\n",
      "                                        if y > 419.000000:\n",
      "                                            if x > 118.000000:\n",
      "                                                return True\n",
      "                                            else:\n",
      "                                                if y > 420.000000:\n",
      "                                                    return True\n",
      "                                                else:\n",
      "                                                    if x > 115.000000:\n",
      "                                                        return False\n",
      "                                                    else:\n",
      "                                                        return True\n",
      "                                        else:\n",
      "                                            return True\n",
      "                    else:\n",
      "                        return True\n",
      "            else:\n",
      "                if x > 96.000000:\n",
      "                    return False\n",
      "                else:\n",
      "                    if y > 400.000000:\n",
      "                        if x > 3.000000:\n",
      "                            if y > 433.000000:\n",
      "                                if y > 434.000000:\n",
      "                                    return True\n",
      "                                else:\n",
      "                                    return False\n",
      "                            else:\n",
      "                                return False\n",
      "                        else:\n",
      "                            if y > 420.000000:\n",
      "                                return False\n",
      "                            else:\n",
      "                                return True\n",
      "                    else:\n",
      "                        if x > 18.000000:\n",
      "                            return False\n",
      "                        else:\n",
      "                            if x > 8.000000:\n",
      "                                return False\n",
      "                            else:\n",
      "                                return True\n",
      "else:\n",
      "    return False\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98cfd0e950>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGBJJREFUeJzt3W2MZFWdx/Hvb5iB+NAOD8qQnUFwWYIw4wKa4IgByoco\nSCK8IWDiAywmJGBkZWMc3BfMvtjNYmJkCdF5sawZzC7gogImuw4SbNsNA2hgYJgBRFcXBpwWF2an\nxYTYM/99UbfGmqG6u6r73rr33PP7JJ2uvn27+tapOv86D/9zShGBmZnlYVndF2BmZuPjoG9mlhEH\nfTOzjDjom5llxEHfzCwjDvpmZhkZKuhL+rWkxyU9JumR4thRku6T9IykLZJW9p1/s6RnJW2TdEZV\nF29mZqMZtqW/H+hExJkRcVZxbANwf0ScAjwAXA8g6QLgpIg4GbgK2FTyNZuZ2SING/Q14NyLgM3F\n7c3Fz73jtwFExMPASkmrlnidZmZWgmGDfgBbJP1U0meLY6siYhogInYDxxbHVwPP9/3tC8UxMzOr\n2fIhzzs7InZLehtwn6Rn6L4RDKIBx7zXg5lZAwwV9IuWPBHxkqS7gbOAaUmrImJa0nHAb4vTdwHH\n9/35GuDFQ+9Tkt8IzMwWISIGNa6HsuDwjqQ3SnpzcftNwEeA7cC9wOXFaZcD9xS37wU+XZy/HtjT\nGwYacOGt/Nq7N3jwwe73Yc6/4YYbar/mpnwlVxZ79xKnn06sWNH9vndvvmXh18VYvpZqmJb+KuB7\nRct8OfCvEXGfpJ8B35b0V8BzwCVFIP8PSR+T9AvgVeCKJV9lQmZm4JxzYMcOWLsWfvITmJio+6qs\nMhMT3Se594Qv4cmemYEnn4R16/yaGYbLa3EWDPoR8Svgdbn2EfEy8OE5/uZzS7+0ND35ZLf+z87C\nzp3d2+vX131VVqmJiSU/yYMaCzY3N64WzytyS7ZuXfdFuGIFnHZa9/ZCOp1O5deVilzLYlBjIdey\nGOTQshhUXjYclTFGtKh/LEVd/7tqMzOl9PYtI72W686d3caCW67zy7m8JBFLmMh10DcbRYUDyW4s\njCbX8nLQNxsXDyQfxBOp9ZTBUoN+c8b0Z2Zg69bud7Mm8kDyAb33v3PP7X7PsdqmWgbNCPqplp5V\nr0mNgcXM0reU3//SLYNmBP1US8+q1bTGQC8nf2oqy6Gd/vdfv/+lWwbNGNPPeSre5rZ1azfgz852\na9bUlBc91GSudQQ5TqT2q2MyuT0TublOxdvc3BhohJkZuP12uPpq2LfP7791a0/QNxvEjYFa9bfw\nly/vdrqcuFQvB30zq8yhI2xf/zpceqkDfp3ak7JpZo1z6GSlA3763NI3s3l5hK1ZPLxjZpYRD+/Y\ngpq0vsnM6uWg31QlReqmrW8ys3o56DdRiZHai53NrJ+DfhOVGKlTXSpuNfFYYOt5IreJSl6J6uwL\nG4q3jk6Cs3faypHaxs17HSXBQd/MyuG9jpLgoG9m5XEPs/Ec9M3MMuLFWWZmNjQHfTNrHWeezs1B\n38xaxavQ5+egb2at4lXo83PQN7NW8Sr0+Tl7x8xap+7M05mZbo9j3bry/79TNs3MGqTq3Sycsmlm\n1iBNn1Nob9BPOWcr5Ws3y1zT5xTaObyT8m6BKV+7mQHVzil4eGeQpvev5pPytZsZ0A3069c3s73W\nzqDf9P7VfFK+djNrvHYO70D9OVtLkfK1m1mlxpayKWkZ8DNgV0R8XNKJwB3AUcCjwKciYlbS4cBt\nwHuA3wGXRsRzA+7PKZtmVaoyWdxqM84x/WuBnX0/3wh8NSJOAfYAVxbHrwRejoiTgZuAryz24rLg\nTB2rgjegOYir2Z8MFfQlrQE+Bvxz3+EPAt8pbm8GLi5uX1T8DHAX8KE57zj3Z8AV06rihIADXM0O\nNmxL/2vAF4EAkHQM8EpE7C9+vwtYXdxeDTwPEBH7gD2Sjh54r7k/A66YVhUnBBzganaw5QudIOlC\nYDoitknq9A4XX/2i73cH3UXf7w6ycft2+MIXYM0aOp0OnU5n0Gnt1auYvc8kzbhiWskmJrprPFJL\nCKhgHiL1ajY5Ocnk5GRp97fgRK6kfwA+CcwCbwAmgLuBjwDHRcR+SeuBGyLiAkk/KG4/LOkw4DcR\nceyA+404/XQvPnKmjllXhQsTh65mCUx+j3XDNUnnAX9TZO/cCXw3Iu6U9A3g8YjYJOlqYF1EXC3p\nMuDiiLhswH1F7N3b2II1szHburU78D472x2WmprqrnAal0RWw9e5IncDcJ2knwNHA7cWx28F3irp\nWeCvi/MGa2CBmllN6p6HyGTwv72Ls8wWI4HufavVOdzZa+n3Bv9b2tJ30DfrSaR7bxVKYI7NQd+s\nLHWPKZsNwbtsWquNdSVl3WPKZmPglr41Vi2jLQl07y1vbumnzpuCzKmWZIomb4RuVgIH/XGYK7B7\nU5B5ebTFrHwO+lWbL7Bnkhe8WL2dBKamnEiTFPdeG81Bv2rzBXY3ZRfk0ZbEuPfaeA76VZsvsLsp\na23j3mvjOXtnHJwRYrlIZFVryrw4y/LmbROax42cSjnoW768bYJlyHn6VomFEjAG/b6MpI2R7sPj\nx2Yjc9C311koAWPQ78tI2hj5PsrMfppvLYXTD61FHPTtdRZqQA/6fRmN7pHvo6zsp7nebZx+aC3k\noG+vs1ADetDvy2h0L+o+ykjkn+vdxsNH1kKeyLWBFkrAGPT7MpI2akn8mCvN0OmH1kDO3jErw1zv\nNk4/tIZx0DdbiHP5rUWcsmk2H0/Gmh3EQd/azZOxZgdx0Ld2806mZgfxmL61nydjrUU8kWtmlhFP\n5JqZ2dAc9M3MMuKgb9Zm3jDODuGgb7ZEjY2rXqNgAzjomy1Bo+Oq1yhUr7Hv+HNz0DdbgkbHVa9R\nqFaj3/Hn5qBvtgSNjqtlfd6ADdbod/y51Z6n772wLHVe+5WpmrbeTnpx1t694c+1NrN01fCOn3TQ\nf/DB4Nxzu72jFSu6vdD162u5HDOzJCS9IrfR46FmZi20YEtf0hHAFHA4sBy4KyL+TtKJwB3AUcCj\nwKciYlbS4cBtwHuA3wGXRsRzA+73wJi+x0PNzIZTeUs/Il4DPhARZwJnABdIei9wI/DViDgF2ANc\nWfzJlcDLEXEycBPwlfnuv4zPtTYzs+EMNbwTEX8obh5Bt7UfwAeA7xTHNwMXF7cvKn4GuAv4UClX\namZmSzZU0Je0TNJjwG7gh8AvgT0Rsb84ZRewuri9GngeICL2AXskHV3qVc8nwRVyZlajzGLGsC39\n/cXwzhrgLODUQacV3w8da1Lf76qV6Ao5S0NrY0NrH9gQMowZy0c5OSL2SvoxsB44UtKyorW/Bnix\nOG0XcDzwoqTDgLdExCuD7m/jxo0Hbnc6HTqdzsgP4CCDVsg5B9RK0IsNrVtT0toHNqQEYsbk5CST\nk5Ol3d8w2TtvBf4YEf8n6Q3AFuAfgc8A342IOyV9A3g8IjZJuhpYFxFXS7oMuDgiLhtwv+V/clZN\nK+Ss/bZupZ1rSlr7wIaUYMyofHGWpHfRnZhdVnzdGRF/L+kd/Cll8zHgkxHxxyLF81vAmcD/ApdF\nxK8H3G81H5foHFCrQIKxYTitfWAjSCxmJL0i15+RaylJLDYMry0PLJONvBz0zcwymptIehsGs8bL\nObMlJYluc1yHrIK+66+NJMN0voFSqDjeyGto2QR9118bmVuPC1ecprwh+ANjhpZN0Hf9tZG59Th/\nxWlaS8obeQ0lm6A/VP1tSqvFmsGtx/krjltSScoqe2fezLSMZv/NRjJXxXGOfy2cslmW3Fcmmi1G\nW3L8E+KgXxa3WswsAQ76ZXKrxcwazkHfLDeZbDdgg3lFrllOmpYmaclx0DdLidMkbYkc9M1S4gVj\ntkQe0zdLjRMOsuaJXDPLhyexPZFrZpnwJHYpHPTNLA2exC6Fg76ZpcGT2KXwmL6ZVaOK8XdPYnsi\n18wayLvWVsYTuWbWPB5/bywHfRsrf05NJjz+3lge3rGxcY8/Mx5/r4SHdywZ7vGnZcm9Mn9mbSM5\n6NvYuMefDq+Daq96g74Hd7PizxlPh3tlfVo2EVXvmP7y5R7cNWsgf3pooYETUWmP6bsZYQlqWcNv\nIPfKCi3s8tQb9FMZ3M2hlttQchrr9jwsrZyIqjfol9GMqDog51TLbUEtbPjZfFrY5ak36C+1GTGO\ngOxabn1a2PCzhbSsy5N2yuY4ArJrufVpYcPPMpP2itxxpRh4ZaGZNYR32awiIPsj2cysodJO2SxD\n2eNtnrhtPmdTNYOfhyQtGPQlrZH0gKSdkrZL+nxx/ChJ90l6RtIWSSv7/uZmSc9K2ibpjCofQOkS\nmrjNss75TbkZ/Dwka5iW/ixwXUScBrwPuEbSO4ENwP0RcQrwAHA9gKQLgJMi4mTgKmBTJVdelUQm\nbrOtcwm9Kbean4dkLRj0I2J3RGwrbv8eeApYA1wEbC5O21z8TPH9tuL8h4GVklaVfN3VSSQ9I9s6\nl8ibcuv5eahM1T345aOcLOlE4AzgIWBVRExD941B0rHFaauB5/v+7IXi2PRSL3ZsevMEDdarc73E\npWzqXO9N2dlU9fLzUIlxbPUzdNCX9GbgLuDaiPi9pLlSbwbNKvvTUkqWdZ1L4E05C34eSjeoB192\nEQ8V9CUtpxvwvxUR9xSHpyWtiohpSccBvy2O7wKO7/vzNcCLg+5348aNB253Oh06nc5IF5871zlr\nDadJA4N78JOTk0xOTpb2P4bK05d0G/C7iLiu79iNwMsRcaOkDcCREbFB0seAayLiQknrgZsi4nWh\nyR+XaGZAI7cvrtNCS48qX5wl6f3AFLCd7jBNAF8GHgG+TbdV/xxwSUTsKf7mFuB84FXgioh4dMD9\nOuibWXfW8txzu2MaK1Z0kyjchZ2TV+SaWdr8iS0jcdA3s/R5f6vXm2Oew0HfzKxt5pnn8N47ZmZt\nU+HqSwd9M7OmqXDF80grcs3MbAwGrL7sDfEvlcf0zcwarn+If3bWY/pmZq3WP8S/VA76ZmYN1z/E\nv1QO+laPLD8Bxmxx+nd8XyqP6dv4ea8Vs0Vznr6lJ9tPgEmQe2St46Bv4+dPXUpDtp/J2W4e3rF6\neK+V5vPul43kvXfMrBre/bKRHPTNrDrukTWOJ3Kt3eqeSKz7/9et95mcmQX8Nj/tDvrWXDVPJM68\nOMOr7z6H8ERmVto+f+2gb81VY2rnzAxcc96TrPjFDjQ7Szi1NBttzyh20C9Bm7uCtaoxtfPJJ+H7\nv1rHTtbyGiv4wwlOLc1F2zOKPZG7RF5cWrGaJhJ7z+tzO2a48MQdfP3Ha5n4Mz+xuWjy/LWzd2rm\nVOb2anLFt3w56NfMqcxmNk4O+g3gFmHJeh8RtG5dswq0qddlWXHQt3Zp6iRJU6/LsuPFWdYuTc2X\na+p1mY3IQd+apan5ck29LrMReXjHmqepkyRNvS7Lisf0zcwy4jF9MzMbmoO+mVlGHPTNzDLioG9m\nlhEHfTMbnbeWTZaDvpmNpu2fMtJyDvpmNhqvTk6ag76Zjcark5O2YNCXdKukaUlP9B07StJ9kp6R\ntEXSyr7f3SzpWUnbJJ1R1YWbWU0mJrobzk1NeeO5BA3T0v8m8NFDjm0A7o+IU4AHgOsBJF0AnBQR\nJwNXAZtKvFZbiCfXbFwmJrqfFtQf8P36S8KCQT8i/gt45ZDDFwGbi9ubi597x28r/u5hYKWkVeVc\nqs3Lk2tWJ7/+krHYMf1jI2IaICJ2A8cWx1cDz/ed90JxzKrmyTWrk19/yVhe8v0N2gRozl3VNm7c\neOB2p9Oh0+mUfDkZ6U2u9T630ZNrNk5+/VVmcnKSycnJ0u5vqF02JZ0AfD8i/rL4+SmgExHTko4D\nfhQRp0raVNy+szjvaeC8Xq/gkPv0Lptl89a/Vie//sZiXLtsioNb8fcClxe3Lwfu6Tv+6eLC1gN7\nBgV8q8igyTWzcfHrLwkLtvQl/RvQAY4BpoEbgLuBfweOB54DLomIPcX5twDnA68CV0TEo3Pcr1v6\nZmYj8oeomJllxB+iYmZmQ3PQN2sKL26yMWhn0HflsdR4cZONSfuCviuPpaiOxU1uHGWpfUHfKwMt\nRePeudKNo2y1L+h721dL0bh3rnTjKFvtTNn0ykCz+fVa+r1tE7xFcjKcpz/AzEy3IbNunV/HVrMm\nvxjdOEqSg/4heg2Y3mvZDRirjV+MVgEvzjqEhyqtMfxitAZqXdD3PK41hl+M4+HU05G0bngHPFRp\nDeIXY7UyHELzmL6Z5Wvr1u5ag9nZbo9qaqq7vXOLeUzf8uaufd48hDYyt/QtXRl27W2AzIbQPLxj\n+cqwa2/m4R3Ll7v2ZiNzS9/SllnX3szDO2ZmGfHwjpmZDc1B38ysLAmkEDvom5mVIZEPpnHQNzMr\nQyIb7Dno2/gk0PU1W7REUoidvWPj4dWzloMxpBA7e8fSUFXX170Ha5KJie6q8AY3aBz0bTyq6Pom\nMnFm1iQe3rHxKbvr6713LEPpD+/U3T2v+//npOyubyITZ7YA18Gxqrelv3dvvZN7nlxMn/feSZvr\n4MjSbunXndda9/+3pUtg4szmkWIdTLxnUm/Qr7t7Xvf/N8tdanWwBckD9U/k1t09r/v/m+UupTrY\ngOQBb61sthgzM92hhXXrmh9orDl6Lf2dO7s9kxrmIBz0zUblyUNbipp7Jo2cyJV0vqSnJf1c0peq\n+B9mi5bi5KHNr29ytfJ51sSTB0oP+pKWAbcAHwXWAp+Q9M6y/0+bTE5O1n0JjUlIGEtZDJo8bEoB\n9GnC66Ip5i2LvsnVfWefw0fPnkl5nrVyVbT0zwKejYj/iYg/AncAF1Xwf1qj7srdpISEsZTFxER3\nSGdqqvsdmlMAfep+XTTJvGXR13PT0ztZ9tQOd+LmUUXQXw083/fzruKYNVSWox39XfQsC6BF+npu\n8c7T2H/q2mQyQOuwvIL7HDTB4BnbBuvVmV5CQnYVJfsCSFyv57ZjB4etXcsWJpLJAK1D6dk7ktYD\nGyPi/OLnDUBExI2HnOc3AjOzRWhUyqakw4BngA8BvwEeAT4REU+V+o/MzGxkpQ/vRMQ+SZ8D7qM7\nZ3CrA76ZWTPUtjjLzMzGr5YN13JbvCXpVknTkp7oO3aUpPskPSNpi6SVfb+7WdKzkrZJOqOeqy6f\npDWSHpC0U9J2SZ8vjudYFkdIeljSY0VZ3FAcP1HSQ0VZ3C5peXH8cEl3FGWxVdLb630E5ZO0TNKj\nku4tfs6yLCT9WtLjxWvjkeJYaXVk7EE/08Vb36T7ePttAO6PiFOAB4DrASRdAJwUEScDVwGbxnmh\nFZsFrouI04D3AdcUz312ZRERrwEfiIgzgTOACyS9F7gR+GpRFnuAK4s/uRJ4uSiLm4Cv1HDZVbsW\n2Nn3c65lsR/oRMSZEXFWcay8OhIRY/0C1gP/2ffzBuBL476OGh73CcATfT8/Dawqbh8HPFXc3gRc\n2nfeU73z2vYF3A18OPeyAN4I/IzuwsbfAsuK4wfqCvAD4L3F7cOAl+q+7pLLYA3wQ6AD3FsceynT\nsvgVcMwhx0qrI3UM73jxVtexETENEBG7gWOL44eWzwu0sHwknUi3hfsQ3RdpdmVRDGc8BuymG/B+\nCeyJiP3FKf1140BZRMQ+YI+ko8d8yVX6GvBFijU9ko4BXsm0LALYIumnkj5bHCutjlSxOGshXrw1\nv9aXj6Q3A3cB10bE7+dZs9HqsigC2pmS3gJ8Dzh10GnF90PLQrSkLCRdCExHxDZJnd5hXv+YW18W\nhbMjYrektwH3SXqGuR/fyHWkjpb+LqB/4mUN8GIN11G3aUmrACQdR7dbD93yOb7vvFaVTzEZdxfw\nrYi4pzicZVn0RMRe4Md0hzCOLOa94ODHe6AsirUwb4mIV8Z9rRV5P/BxSf8N3A58kO5Y/coMy6LX\nkiciXqI7BHoWJdaROoL+T4G/kHSCpMOBy4B7a7iOcTu05XIvcHlx+3Lgnr7jn4YDq5v39Lp1LfEv\nwM6I+Ke+Y9mVhaS39jIwJL2B7tzGTuBHwCXFaZ/h4LL4THH7ErqTea0QEV+OiLdHxJ/TjQcPRMQn\nybAsJL2x6Akj6U3AR4DtlFlHapqoOJ/uqt1ngQ11T5yM4fH+G91339eA54ArgKOA+4ty+CFwZN/5\ntwC/AB4H3l339ZdYDu8H9gHbgMeAR4vXwtEZlsW7ise/DXgC+Nvi+DuAh4GfA3cCK4rjRwDfLurM\nQ8CJdT+GisrlPP40kZtdWRSPuVc/tvfiY5l1xIuzzMwyUsviLDMzq4eDvplZRhz0zcwy4qBvZpYR\nB30zs4w46JuZZcRB38wsIw76ZmYZ+X/qFGSdv8ttwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f990c287e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from learn_interactive import DecisionTree, make_random_tree, canvas_dim\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "real_tree = make_random_tree()\n",
    "print real_tree\n",
    "\n",
    "n_points = 100\n",
    "noise = 0.2\n",
    "\n",
    "# generate some noisy labels\n",
    "points = np.random.randint(0, canvas_dim, (n_points, 2))\n",
    "labels = np.zeros((points.shape[0],),dtype=np.bool_)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label, _ = real_tree.evaluate(points[i,0], points[i,1])\n",
    "    if np.random.rand() < noise:\n",
    "        label = not label\n",
    "    labels[i] = label\n",
    "\n",
    "# plot the points (blue for positive, red for negative)\n",
    "plt.plot(points[labels==1,0], points[labels==1,1],'b.')\n",
    "plt.plot(points[labels==0,0], points[labels==0,1],'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity as a Metric for Best Split\n",
    "\n",
    "In order to talk about the \"best split\", we have to have some notion of what \"best\" means.  There are lots of [possible options](https://en.wikipedia.org/wiki/Decision_tree_learning).  The one we are going to examine here is called \"Gini impurity\".  The Gini impurity is a measure of the homegeneity of the points in a set.  Suppose that we have a set of $x_1, \\ldots, x_n$ with target values $y_1, \\ldots, y_n$.  For simplicity, we will assume that each of the target values $y_i \\in \\{0, 1\\}$ (i.e. it is a binary variable).  The Gini impurity of the set is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mbox{gini_impurity}(y_1, \\ldots, y_n) =& f_0 \\times (1 - f_0) + f_1 \\times (1 - f_1)\n",
    "\\end{align}\n",
    "\n",
    "Where $f_c = \\frac{1}{n}\\sum_{i=1}^n y_i = c$.  In otherwords $f_c$ is the proportion of points in the set that belong to class $c$.  Since we are dealing with binary target variables, $c$ in this case will either be $0$ or $1$.\n",
    "\n",
    "**Q:** What is the highest value that gini-impuritycan obtain?  What is the lowest?\n",
    "\n",
    "In order to relate this back to the concept of the best possible split to our data, consider a possible split of our data.  Specifically, we will consider splits that arise from testing one of the two attributes for each data point (x or y) and comparing it to a threshold (e.g. $x > 50$ or $y > 100$).  Let $y^{p}_1, \\ldots, y^{p}_{n_{p}}$  denote the targets for the points that passed the Boolean test, and let $y^{f}_1, \\ldots, y^{f}_{n_{f}}$ denote the targets for the points that failed the Boolean test.  The impurity of this split is given by the following formula.\n",
    "\n",
    "\\begin{align}\n",
    "\\mbox{split_impurity}(y_1, \\ldots, y_n) =& \\frac{n_p}{n}\\mbox{gini_impurity}(y^{p}_1, \\ldots, y^{p}_{n_{p}}) + \\frac{n_f}{n}\\mbox{gini_impurity}(y^{f}_1, \\ldots, y^{f}_{n_{f}})\n",
    "\\end{align}\n",
    "\n",
    "That is, the impurity of a split is a weighted combination of the Gini impurity of each resultant subset weighted by the proportion of data that is assigned to each subset.\n",
    "\n",
    "Fill out the following function stub to compute the best possible split of the data using Gini impurity as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_split(data, targets):\n",
    "    \"\"\" Find the best split for a given dataset.\n",
    "        data is an nx2 numpy array where n is the number of data points.  The first column\n",
    "        of data contains the x coordinates of the points, and the second contains the y\n",
    "        coordinates.\n",
    "        targets is an n dimensional numpy array containing the binary target values (0, 1)\n",
    "        \n",
    "        returns: a tuple consisting of the variable to split on and the threshold to split at.\n",
    "                 all splits are evaluated as variable_name > threshold.\n",
    "        \"\"\"\n",
    "    # this starter code is designed to help, but if you want to start from scratch, please\n",
    "    # go ahead.\n",
    "    best_impurity = np.inf\n",
    "    split_variable = None\n",
    "    split_threshold = None\n",
    "    variable_names = ['x', 'y']\n",
    "    for i in range(data.shape[1]):\n",
    "        for threshold in np.arange(-1.0, canvas_dim+1, 1):\n",
    "            # loop over all possible splitting variables and thresholds\n",
    "            pass\n",
    "\n",
    "split_variable, split_threshold = best_split(points, labels)\n",
    "print split_variable, split_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize the best split computed above\n",
    "\n",
    "plt.plot(points[labels==1,0], points[labels==1,1],'b.')\n",
    "plt.plot(points[labels==0,0], points[labels==0,1],'r.')\n",
    "if split_variable == \"x\":\n",
    "    plt.plot([split_threshold, split_threshold], [0, canvas_dim], 'k')\n",
    "else:\n",
    "    plt.plot([0, canvas_dim],[split_threshold, split_threshold], 'k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
